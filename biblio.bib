@article{asdf,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  archivePrefix = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Wed, 07 Jun 2017 14:43:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MnihKSGAWR13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Williams92simplestatistical,
    author = {Ronald J. Williams},
    title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {229--256}
}
@inproceedings{Sutton:1999:PGM:3009657.3009806,
 author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
 title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
 booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
 series = {NIPS'99},
 year = {1999},
 location = {Denver, CO},
 pages = {1057--1063},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3009657.3009806},
 acmid = {3009806},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA}
} 

@InProceedings{natural,
  author =       "Kakade, Sham",
  title =        "A Natural Policy Gradient",
  booktitle =    "Advances in Neural Information Processing Systems 14 (NIPS 2001)",
  editor =    "Dietterich, Thomas G. and Becker, Suzanna and Ghahramani, Zoubin",
  year =         "2001",
  publisher = "MIT Press",
  pages =     "1531-1538",
  url = "http://books.nips.cc/papers/files/nips14/CN11.pdf",
  bib2html_rescat = "Learning Methods, MDP"
}

@article{peters,
  title = {Reinforcement Learning of Motor Skills with Policy Gradients},
  author = {Peters, J. and Schaal, S.},
  journal = {Neural Networks},
  volume = {21},
  number = {4},
  pages = {682-697},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  month = may,
  year = {2008},
  month_numeric = {5}
}
@incollection{adaptive_step,
title = {Adaptive Step-Size for Policy Gradient Methods},
author = {Pirotta, Matteo and Restelli, Marcello and Bascetta, Luca},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {1394--1402},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5186-adaptive-step-size-for-policy-gradient-methods.pdf}
}
@incollection{adaptive_batch,
title = {Adaptive Batch Size for Safe Policy Gradients},
author = {Papini, Matteo and Pirotta, Matteo and Restelli, Marcello},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {3591--3600},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6950-adaptive-batch-size-for-safe-policy-gradients.pdf}
}

@InProceedings{safe_iteration,
  title = 	 {Safe Policy Iteration},
  author = 	 {Matteo Pirotta and Marcello Restelli and Alessio Pecorino and Daniele Calandriello},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {307--315},
  year = 	 {2013},
  editor = 	 {Sanjoy Dasgupta and David McAllester},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/pirotta13.pdf},
  url = 	 {http://proceedings.mlr.press/v28/pirotta13.html},
  abstract = 	 {This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.  When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.  To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.  We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.  Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on some chain-walk domains and on the Blackjack card game.}
}

@ARTICLE{trpo,
   author = {{Schulman}, J. and {Levine}, S. and {Moritz}, P. and {Jordan}, M.~I. and 
	{Abbeel}, P.},
    title = "{Trust Region Policy Optimization}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1502.05477},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2015,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150205477S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017arXiv170804133I,
   author = {{Islam}, R. and {Henderson}, P. and {Gomrokchi}, M. and {Precup}, D.
	},
    title = "{Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1708.04133},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2017,
    month = aug,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170804133I},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015arXiv150902971L,
   author = {{Lillicrap}, T.~P. and {Hunt}, J.~J. and {Pritzel}, A. and {Heess}, N. and 
	{Erez}, T. and {Tassa}, Y. and {Silver}, D. and {Wierstra}, D.
	},
    title = "{Continuous control with deep reinforcement learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1509.02971},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Statistics - Machine Learning},
     year = 2015,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150902971L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{DBLP:journals/corr/abs-1709-06560,
  author    = {Peter Henderson and
               Riashat Islam and
               Philip Bachman and
               Joelle Pineau and
               Doina Precup and
               David Meger},
  title     = {Deep Reinforcement Learning that Matters},
  journal   = {CoRR},
  volume    = {abs/1709.06560},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.06560},
  archivePrefix = {arXiv},
  eprint    = {1709.06560},
  timestamp = {Tue, 17 Oct 2017 12:29:59 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-06560},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1802-04412,
  author    = {Kamyar Azizzadenesheli and
               Emma Brunskill and
               Animashree Anandkumar},
  title     = {Efficient Exploration through Bayesian Deep Q-Networks},
  journal   = {CoRR},
  volume    = {abs/1802.04412},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.04412},
  archivePrefix = {arXiv},
  eprint    = {1802.04412},
  timestamp = {Thu, 01 Mar 2018 15:00:45 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-04412},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Dearden:1998:BQ:295240.295801,
 author = {Dearden, Richard and Friedman, Nir and Russell, Stuart},
 title = {Bayesian Q-learning},
 booktitle = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
 series = {AAAI '98/IAAI '98},
 year = {1998},
 isbn = {0-262-51098-7},
 location = {Madison, Wisconsin, USA},
 pages = {761--768},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=295240.295801},
 acmid = {295801},
 publisher = {American Association for Artificial Intelligence},
 address = {Menlo Park, CA, USA}
}


@article{DBLP:journals/corr/abs-1712-03632,
  author    = {Anay Pattanaik and
               Zhenyi Tang and
               Shuijing Liu and
               Gautham Bommannan and
               Girish Chowdhary},
  title     = {Robust Deep Reinforcement Learning with Adversarial Attacks},
  journal   = {CoRR},
  volume    = {abs/1712.03632},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.03632},
  archivePrefix = {arXiv},
  eprint    = {1712.03632},
  timestamp = {Wed, 03 Jan 2018 12:33:17 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-03632},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/PintoDSG17,
  author    = {Lerrel Pinto and
               James Davidson and
               Rahul Sukthankar and
               Abhinav Gupta},
  title     = {Robust Adversarial Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1703.02702},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.02702},
  archivePrefix = {arXiv},
  eprint    = {1703.02702},
  timestamp = {Wed, 07 Jun 2017 14:41:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/PintoDSG17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
} 


@Article{Bertsekas2011,
author="Bertsekas, Dimitri P.",
title="Approximate policy iteration: a survey and some new methods",
journal="Journal of Control Theory and Applications",
year="2011",
month="Aug",
day="01",
volume="9",
number="3",
pages="310--335",
issn="1993-0623",
doi="10.1007/s11768-011-1005-3",
url="https://doi.org/10.1007/s11768-011-1005-3"
}

@incollection{NIPS2011_4274,
title = {A reinterpretation of the policy oscillation phenomenon in approximate policy iteration},
author = {Wagner, Paul},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
pages = {2573--2581},
year = {2011},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4274-a-reinterpretation-of-the-policy-oscillation-phenomenon-in-approximate-policy-iteration.pdf}
}

@article{DBLP:journals/corr/abs-1712-03428,
  author    = {Matteo Pirotta and
               Marcello Restelli},
  title     = {Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent},
  journal   = {CoRR},
  volume    = {abs/1712.03428},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.03428},
  archivePrefix = {arXiv},
  eprint    = {1712.03428},
  timestamp = {Wed, 03 Jan 2018 12:33:17 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-03428},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@InProceedings{pmlr-v37-thomas15,
  title = 	 {High Confidence Policy Improvement},
  author = 	 {Philip Thomas and Georgios Theocharous and Mohammad Ghavamzadeh},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2380--2388},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/thomas15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/thomas15.html}
}

@inproceedings{Precup:2000:ETO:645529.658134,
 author = {Precup, Doina and Sutton, Richard S. and Singh, Satinder P.},
 title = {Eligibility Traces for Off-Policy Policy Evaluation},
 booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
 series = {ICML '00},
 year = {2000},
 isbn = {1-55860-707-2},
 pages = {759--766},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=645529.658134},
 acmid = {658134},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA}
} 

@inproceedings{Petrik:2016:SPI:3157096.3157354,
 author = {Petrik, Marek and Ghavamzadeh, Mohammad and Chow, Yinlam},
 title = {Safe Policy Improvement by Minimizing Robust Baseline Regret},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {2306--2314},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157354},
 acmid = {3157354},
 publisher = {Curran Associates Inc.},
 address = {USA}
} 

@INPROCEEDINGS{Kakade02approximatelyoptimal,
    author = {Sham Kakade and John Langford},
    title = {Approximately Optimal Approximate Reinforcement Learning},
    booktitle = {IN PROC. 19TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING},
    year = {2002},
    pages = {267--274},
    publisher = {}
}






@article{10.2307/3689239,
 ISSN = {0364765X, 15265471},
 URL = {http://www.jstor.org/stable/3689239},
 abstract = {The policy iteration method of dynamic programming is studied in an abstract setting. It is shown to be equivalent to the Newton-Kantorovich iteration procedure applied to the functional equation of dynamic programming. This equivalence is used to obtain the rate of convergence and error bounds for the sequence of values generated by policy iteration. These results are discussed in the context of the finite state Markovian decision problem with compact action space. An example is analyzed in detail.},
 author = {Martin L. Puterman and Shelby L. Brumelle},
 journal = {Mathematics of Operations Research},
 number = {1},
 pages = {60--69},
 publisher = {INFORMS},
 title = {On the Convergence of Policy Iteration in Stationary Dynamic Programming},
 volume = {4},
 year = {1979}
}


@ARTICLE{6796861,
author={T. Jaakkola and M. I. Jordan and S. P. Singh},
journal={Neural Computation},
title={On the Convergence of Stochastic Iterative Dynamic Programming Algorithms},
year={1994},
volume={6},
number={6},
pages={1185-1201},
keywords={},
doi={10.1162/neco.1994.6.6.1185},
ISSN={0899-7667},
month={Nov},}


@article{Tsitsiklis:2003:COP:944919.944922,
 author = {Tsitsiklis, John N.},
 title = {On the Convergence of Optimistic Policy Iteration},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {59--72},
 numpages = {14},
 url = {https://doi.org/10.1162/153244303768966102},
 doi = {10.1162/153244303768966102},
 acmid = {944922},
 publisher = {JMLR.org},
 keywords = {Markov decision problem, Monte Carlo, Stochastic approximation, dynamic programming, reinforcement learning, temporal differences}
} 

@article{BELLMAN1958228,
title = "Dynamic programming and stochastic control processes",
journal = "Information and Control",
volume = "1",
number = "3",
pages = "228 - 239",
year = "1958",
issn = "0019-9958",
doi = "https://doi.org/10.1016/S0019-9958(58)80003-0",
url = "http://www.sciencedirect.com/science/article/pii/S0019995858800030",
author = "Richard Bellman"
}

@book{howard:dp,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, MA},
  author = {Howard, R. A.},
  biburl = {https://www.bibsonomy.org/bibtex/28b55f737ee6dd7800ffc7952a33bb6bd/idsia},
  citeulike-article-id = {2380352},
  interhash = {7eed9f4f6bd1f9ee063d80d0f732e48f},
  intrahash = {8b55f737ee6dd7800ffc7952a33bb6bd},
  keywords = {inaki},
  priority = {2},
  publisher = {MIT Press},
  timestamp = {2008-03-11T14:56:12.000+0100},
  title = {Dynamic Programming and Markov Processes},
  year = 1960
}







@Article{Watkins1992,
author="Watkins, Christopher J. C. H.
and Dayan, Peter",
title="Q-learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="279--292",
abstract="Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
issn="1573-0565",
doi="10.1007/BF00992698",
url="https://doi.org/10.1007/BF00992698"
}
@Article{Sutton1988,
author="Sutton, Richard S.",
title="Learning to Predict by the Methods of Temporal Differences",
journal="Machine Learning",
year="1988",
month="Aug",
day="01",
volume="3",
number="1",
pages="9--44",
abstract="This article introduces a class of incremental learning procedures specialized for prediction -- that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",
issn="1573-0565",
doi="10.1023/A:1022633531479",
url="https://doi.org/10.1023/A:1022633531479"
}
@article{sarsa,
author = {A. Rummery, G and Niranjan, Mahesan},
year = {1994},
month = {11},
pages = {},
title = {On-Line Q-Learning Using Connectionist Systems},
booktitle = {Technical Report CUED/F-INFENG/TR 166}
}


@book{dyer1970computation,
  title={The Computation and Theory of Optimal Control},
  author={Dyer, P. and McReynolds, S.R.},
  isbn={9780122262500},
  lccn={71091433},
  series={Mathematics in Science and Engineering : a series of monographs and textbooks},
  url={https://books.google.it/books?id=A7IwlAEACAAJ},
  year={1970},
  publisher={Academic Press}
}
@book{hasdorff1976gradient,
  title={Gradient optimization and nonlinear control},
  author={Hasdorff, L.},
  isbn={9780471358701},
  lccn={75040187},
  url={https://books.google.it/books?id=o\_ZQAAAAMAAJ},
  year={1976},
  publisher={John Wiley \& Sons Australia, Limited}
}

@article{Glynn:1990:LRG:84537.84552,
 author = {Glynn, Peter W.},
 title = {Likelihood Ratio Gradient Estimation for Stochastic Systems},
 journal = {Commun. ACM},
 issue_date = {Oct. 1990},
 volume = {33},
 number = {10},
 month = oct,
 year = {1990},
 issn = {0001-0782},
 pages = {75--84},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/84537.84552},
 doi = {10.1145/84537.84552},
 acmid = {84552},
 publisher = {ACM},
 address = {New York, NY, USA}
} 

@article{DBLP:journals/corr/abs-1106-0665,
  author    = {Peter L. Bartlett and
               Jonathan Baxter},
  title     = {Infinite-Horizon Policy-Gradient Estimation},
  journal   = {CoRR},
  volume    = {abs/1106.0665},
  year      = {2011},
  url       = {http://arxiv.org/abs/1106.0665},
  archivePrefix = {arXiv},
  eprint    = {1106.0665},
  timestamp = {Wed, 07 Jun 2017 14:41:14 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1106-0665},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{rade2000springers,
  title={Springers mathematische Formeln: Taschenbuch f{\"u}r Ingenieure, Naturwissenschaftler, Informatiker, Wirtschaftswissenschaftler},
  author={Rade, L. and Vachenauer, P. and Westergren, B.},
  isbn={9783540675051},
  url={https://books.google.it/books?id=DICwim5DphgC},
  year={2000},
  publisher={Springer Berlin Heidelberg}
}

@article{Amari:1998:NGW:287476.287477,
 author = {Amari, Shun-Ichi},
 title = {Natural Gradient Works Efficiently in Learning},
 journal = {Neural Comput.},
 issue_date = {Feb. 15, 1998},
 volume = {10},
 number = {2},
 month = feb,
 year = {1998},
 issn = {0899-7667},
 pages = {251--276},
 numpages = {26},
 url = {http://dx.doi.org/10.1162/089976698300017746},
 doi = {10.1162/089976698300017746},
 acmid = {287477},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA}
} 

@article{PETERS20081180,
title = "Natural Actor-Critic",
journal = "Neurocomputing",
volume = "71",
number = "7",
pages = "1180 - 1190",
year = "2008",
note = "Progress in Modeling, Theory, and Application of Computational Intelligenc",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2007.11.026",
url = "http://www.sciencedirect.com/science/article/pii/S0925231208000532",
author = "Jan Peters and Stefan Schaal",
keywords = "Policy-gradient methods, Compatible function approximation, Natural gradients, Actor-Critic methods, Reinforcement learning, Robot learning"
}

@article{kiefer1952,
author = "Kiefer, J. and Wolfowitz, J.",
doi = "10.1214/aoms/1177729392",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "09",
number = "3",
pages = "462--466",
publisher = "The Institute of Mathematical Statistics",
title = "Stochastic Estimation of the Maximum of a Regression Function",
url = "https://doi.org/10.1214/aoms/1177729392",
volume = "23",
year = "1952"
}


@inproceedings{ziebart_maximum_2008,
	title = {Maximum {Entropy} {Inverse} {Reinforcement} {Learning}.},
	volume = {8},
	booktitle = {{AAAI}},
	publisher = {Chicago, IL, USA},
	author = {Ziebart, Brian D. and Maas, Andrew L. and Bagnell, J. Andrew and Dey, Anind K.},
	year = {2008},
	pages = {1433--1438}
}

@inproceedings{toussaint_robot_2009,
	address = {New York, NY, USA},
	series = {{ICML} '09},
	title = {Robot {Trajectory} {Optimization} {Using} {Approximate} {Inference}},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553508},
	doi = {10.1145/1553374.1553508},
	abstract = {The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot.},
	urldate = {2018-02-20},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Toussaint, Marc},
	year = {2009},
	pages = {1049--1056}
}

@inproceedings{rawlik_stochastic_2013,
	title = {On {Stochastic} {Optimal} {Control} and {Reinforcement} {Learning} by {Approximate} {Inference} ({Extended} {Abstract})},
	copyright = {Authors who submit to this conference agree to the following terms:    a) Authors transfer their copyrights in their paper to the International Joint Conferences on Artificial Intelligence, Inc. (IJCAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights currently exist or hereafter come into effect, and also the exclusive right to create electronic versions of the paper, to the extent that such right is not subsumed under copyright.    b) Every named author warrants that he/she is the sole author and owner of the copyright in the paper, except for those portions shown to be in quotations; that the paper is original throughout; and that their right to make the grants set forth above is complete and unencumbered. If anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, each author, individually and collectively, will hold harmless and indemnify IJCAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense IJCAI may make to such claim or action. Moreover, each author agrees to cooperate in any claim or other action seeking to protect or enforce any right the author has granted to IJCAI in the paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, each author agrees to reimburse whomever brings such claim or action for expenses and attorney\&rsquo;s fees incurred therein.   c) \&nbsp;In return for these rights, IJCAI hereby grants to each author, and the employers for whom the work was performed, royalty-free permission to: 1. retain all proprietary rights (such as patent rights) other than copyright and the publication rights transferred to IJCAI; 2. personally reuse all or portions of the paper in other works of their own authorship; 3. make oral presentation of the material in any forum; 4. reproduce, or have reproduced, the  paper for the author\&rsquo;s personal use, or for company use provided that IJCAI copyright and the source are indicated, and that the copies are not used in a way that implies IJCAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the paper in electronic or digital form on any computer network, except by the author or the author\&rsquo;s employer, and then only on the author\&rsquo;s or the employer\&rsquo;s own World Wide Web page or ftp site. Such Web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the IJCAI electronic server (http://www.ijcai.org), and shall not post other IJCAI copyrighted materials not of the author\&rsquo;s or the employer\&rsquo;s creation (including tables of contents with links to other papers) without IJCAI\&rsquo;s written permission; \&gt;5. make limited distribution of all or portions of the above paper prior to publication. 6. In the case of work performed under U.S. Government contract, IJCAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above paper, and to authorize others to do so, for U.S. Government purposes. In the event the above paper is not accepted and published by IJCAI, or is withdrawn by the author(s) before acceptance by IJCAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6658},
	abstract = {We present a reformulation of the stochastic optimal control problem in terms of KL divergence minimisation, not only providing a unifying perspective of previous approaches in this area, but also demonstrating that the formalism leads to novel practical approaches to the control problem. Specifically, a natural relaxation of the dual formulation gives rise to exact iterative solutions to the finite and infinite horizon stochastic optimal control problem, while direct application of Bayesian inference methods yields instances of risk sensitive control.},
	language = {en},
	urldate = {2018-02-20},
	booktitle = {Twenty-{Third} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
	month = jun,
	year = {2013}
}

@article{fox_taming_2015,
	title = {Taming the {Noise} in {Reinforcement} {Learning} via {Soft} {Updates}},
	url = {http://arxiv.org/abs/1512.08562},
	abstract = {Model-free reinforcement learning algorithms, such as Q-learning, perform poorly in the early stages of learning in noisy environments, because much effort is spent unlearning biased estimates of the state-action value function. The bias results from selecting, among several noisy estimates, the apparent optimum, which may actually be suboptimal. We propose G-learning, a new off-policy learning algorithm that regularizes the value estimates by penalizing deterministic policies in the beginning of the learning process. We show that this method reduces the bias of the value-function estimation, leading to faster convergence to the optimal value and the optimal policy. Moreover, G-learning enables the natural incorporation of prior domain knowledge, when available. The stochastic nature of G-learning also makes it avoid some exploration costs, a property usually attributed only to on-policy algorithms. We illustrate these ideas in several examples, where G-learning results in significant improvements of the convergence rate and the cost of the learning process.},
	urldate = {2018-02-20},
	journal = {arXiv:1512.08562 [cs, math]},
	author = {Fox, Roy and Pakman, Ari and Tishby, Naftali},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.08562},
	keywords = {Computer Science - Information Theory, Computer Science - Learning}
}


@article{haarnoja_reinforcement_2017,
	title = {Reinforcement {Learning} with {Deep} {Energy}-{Based} {Policies}},
	url = {http://arxiv.org/abs/1702.08165},
	abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
	urldate = {2018-02-20},
	journal = {arXiv:1702.08165 [cs]},
	author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08165},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning}
}

@inproceedings{hans2008safe,
  title={Safe exploration for reinforcement learning.},
  author={Hans, Alexander and Schneega{\ss}, Daniel and Sch{\"a}fer, Anton Maximilian and Udluft, Steffen},
  booktitle={ESANN},
  pages={143--148},
  year={2008}
}
@article{moldovan2012safe,
  title={Safe exploration in Markov decision processes},
  author={Moldovan, Teodor Mihai and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1205.4810},
  year={2012}
}
@article{garcia2012safe,
  title={Safe exploration of state and action spaces in reinforcement learning},
  author={Garcia, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Artificial Intelligence Research},
  volume={45},
  pages={515--564},
  year={2012}
}
@inproceedings{munos2016safe,
  title={Safe and efficient off-policy reinforcement learning},
  author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1054--1062},
  year={2016}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@inproceedings{gehring2013smart,
  title={Smart exploration in reinforcement learning using absolute temporal difference errors},
  author={Gehring, Clement and Precup, Doina},
  booktitle={Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems},
  pages={1037--1044},
  year={2013},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}

@inproceedings{nutinicoordinate,
  title={Coordinate descent converges faster with the Gauss-Southwell rule than random selection},
  author={Nutini, Julie and Schmidt, Mark and Laradji, Issam and Friedlander, Michael and Koepke, Hoyt},
  booktitle={International Conference on Machine Learning},
  pages={1632--1641},
  year={2015}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={ICML},
  year={2014}
}


@article{gil2013renyi,
  title={R{\'e}nyi divergence measures for commonly used univariate continuous distributions},
  author={Gil, Manuel and Alajaji, Fady and Linder, Tamas},
  journal={Information Sciences},
  volume={249},
  pages={124--131},
  year={2013},
  publisher={Elsevier}
}
@article{pinsker1960information,
  title={Information and information stability of random variables and processes},
  author={Pinsker, Mark S},
  year={1960},
  publisher={Izv. Akad. Nauk}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}
