\chapter{Balancing Safety and Exploration in Policy Gradient}
\label{ch:balance}
\thispagestyle{empty}

\fancyhead[LE,RO]{\bfseries\thepage}                               
\fancyhead[RE]{\bfseries{CHAPTER 4. BALANCING SAFETY AND EXPLORATION}}    
\fancyhead[LO]{\bfseries\rightmark}  


One of the challenges that arise in reinforcement learning - opposed to other kinds of learning - is the trade-off between exploration and exploitation. To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be effective in producing reward. At the same time, though, discovering such actions requires a trial and error search. The agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. This dilemma is that neither exploration not exploitation can be pursued exclusively without failing at the task: a fully exploit algorithm would likely choose sub-optimal actions while a fully exploratory behaviour will likely to take random actions without long-time awareness.\\
The agent must try a variety of actions and progressively favour those that appear to be the best. This dilemma can be made even harder when we consider stochastic rewards, where the agents must try the same actions multiple times to be sure about its estimated value. The exploration-exploitation dilemma has been studied intensively for many decades, yet remains unsolved. 

In this chapter we will try to combine this important aspect of exploration with safe learning. As it is shown in \ref{ch:safepg} safe learning algorithms tend to be over-conservative, resulting in a slow and inefficient learning. By customizing safety constraints to match the user needs, we will introduce a new practical algorithm named SEPG (Safely-Exploring Policy Gradient) for safe learning. 

\Cref{sec:sepg-intro} will start by analysing the problem and previous solutions. Then we will set up a new framework in \Cref{sec:framework} that reconciles all the aspects of safe learning. In \ref{sec:theory} we will develop new foundations upon which SEPG is based. Finally we will present the final algorithm in \ref{sec:algos}.



In Section \ref{sec:framework} we give a general definition of safety and catalog special cases corresponding to relevant application scenarios; in Section \ref{sec:theory} we generalize existing performance improvement bounds for Gaussian parametric policies in two ways: we take more general safety constraints into consideration, and we deal with adaptive policy variance. Furthermore, we propose a more exploratory policy update; in Section \ref{sec:algos}, we exploit this theoretical analysis to build policy gradient algorithms tailored to the different safe-learning scenarios, providing Safely-Exploring Policy Gradient (SEPG), a very general framework that can be customized to one's needs; finally, Section \ref{sec:exp} empirically compares the proposed algorithms on benchmark continuous RL tasks.



\section{Preliminaries}\label{sec:sepg-prelim}




\section{Exploration in Safe Policy Gradient}\label{sec:sepg-intro}
Exploration is a fundamental aspect of RL (agents that do not value exploration are often excessively greedy, tending to get stuck in sub-optimal behaviours) and is a topical object of study in the recent literature. At the same time, an excessive exploratory behaviour can harm systems and people, other than have an intuitive economical interpretation (\eg an excessive movement can lead to the breaking of a robotic arm).

In this work we aim at finding a good trade-off between these competing aspects, exploration and safety, using policy gradient method. Previous approaches in safe policy gradient, described in \Cref{sec:ass}, employ a Gaussian policy with constant parameter $\sigma$ that allows for some exploration. However the constant value of $\sigma$ is domain-dependent and does not take into account any possibility to adapt to the surrounding environment.\\
One possibility is to extend this approach by updating $\sigma$ as one of the policy parameters: $\sigma \gets \sigma + \nabla_\sigma J_\mu(\pi_{\vtheta,\sigma})$. However this approach has some limitations:
\begin{itemize}
\item The bounds on policy improvement in \cite{adaptive_step} are no longer applicable. For this reason, we are not guaranteed to have safe updates.
\item In many cases (\eg LQG) exploration is highly penalized in the performance value. This penalization will quickly reduce the policy exploration parameter, resulting in an almost deterministic policy. This is not good since a lack of exploration causes a really slow learning process and it will be hard to escape from local maxima.
\end{itemize}

A different approach was taken by~\cite{haarnoja_reinforcement_2017}, \cite{ziebart_maximum_2008} that employed maximum entropy reinforcement learning to improve exploration strategy. A new intrinsic reward is given to the agent according to the entropy of the action distribution in that state. In this way, the agent is encouraged to explore as a way to increase the entropy of the policy. Despite its good result in practice, there are still some aspects that must be taken into consideration:
\begin{itemize}
\item The agent is optimizing a surrogate objective function that is not defined by the problem.
\item This method introduces an additional hyper-parameter (the weight of the entropy value) that has to be tuned for the environment.
\item It is not clear how to guarantee safe updates with new surrogate objective function.
\end{itemize}

We will propose a new method to adapt exploration in safe reinforcement learning in \Cref{sec:explore+safe} and we will show the results of this new approach in \Cref{ch:experim}.


\section{Safe Learning Framework}\label{sec:framework}

As we have seen in \Cref{ch:safepg}, several works have studied the problem of safety in reinforcement learning. However, in \Cref{sec:other-safe} we analysed some works that consider a safety constraint different than the other seen in \Cref{ch:safepg}, which mainly consists in monotonic improvement. 

In this section we will summarize most of the approaches for safe learning by formalizing a new framework where we distinguish between two main categories, each of them allowing for more shades. We identified two main types of safety constraints, described in details below:

\subsection{Type-I safety} 
Type-I safety guarantees that the agent never performs dangerous behaviours. This can be imposed by explicitly defining dangerous states and/or actions~\cite{gehring2013smart} or by designing the reward signal such that danger is matched with low performance~\cite{safe_iteration}\cite{trpo}\cite{Petrik:2016:SPI:3157096.3157354}.\\
This can be accomplished by imposing a condition of the kind:%
%
\begin{equation}
J(\vtheta^{t+1}) \geq \jbase
\end{equation}
for every time step $t$ with respect to a certain baseline $\jbase$. The baseline represents a lower-bound on the policy performance.\\
This condition can be achieved by constraining each parameter update as follows:
\begin{equation}\label{eq:simple-update}
J(\vtheta^{t+1}) - J(\vtheta^{t}) \geq C^t,
\end{equation}
for some $C^t\in \realspace$, where $\jbase[t]=C^t + J(\vtheta^t)$.
\begin{definition}[Required improvement]\label{def:required-improvement}
We define an update in \ref{eq:simple-update} to be a \textit{required improvement} when $C^t \geq 0$. A required improvement happens when we are forced to improve the policy performance by a given quantity $C^t$.
\end{definition}
\begin{definition}[Bounded worsening]\label{def:bounded-worsening}
We define an update in \ref{eq:simple-update} to be a \textit{bounded worsening} when $C^t < 0$. A bounded worsening happens when we can afford to lose at most $|C^t|$ in a single update. The typical case is when we want to exit from a local maxima but we put a constraint on the maximum performance loss.
\end{definition}



Depending on the value of the baseline policy $\jbase[t]$ and, in particular, on the value of $C_t$, we can obtain different properties. Two interesting settings that can be identified are:

\textbf{Monotonic Improvement (MI):} Setting $C^t= 0$ forces a monotonic improvement behaviour, which models the case in which the agent is constrained to continually improve its performance. This is appropriate when policy updates can be done rarely, take a lot of time or have costs/risks that are not modelled in the reward. This is the constraint considered in the safe policy gradient literature seen in \Cref{ch:safepg}.

\textbf{Lower-Bounded I (LB-I):}
Setting $C^t = J(\vtheta^0) - J(\vtheta^t)$ puts a lower bound on the performance that matches the performance of the policy $\pi_{\vtheta^0}$. In this case the main goal is to never do worse than the initial policy. This is appropriate when the initial policy have been designed to avoid dangerous behaviour (modelled in the reward), or when improvement over an already good policy must be guaranteed to justify the deployment of a RL algorithm~\cite{pmlr-v37-thomas15}.



\subsection{Type-II safety}

Type-II safety has an economic connotation. Here reward is designed to record costs and revenues and the constraint is to have sufficient earnings (or limited losses) over some significant horizon. Low performance (\eg due to exploration) is acceptable as long as it is repaid soon enough by an improved behavior~\cite{adaptive_batch}\cite{pmlr-v37-thomas15}.

Formally, type-II safety can be defined as to guarantee a minimum \textit{average} performance $\jbase$ over some interesting time horizon, that we identify with a \textit{learning iteration}. This could be, \eg, a day of production in an automated factory or an accounting trimester and so on. We can run different policies within an iteration, as long as the average performance is above a certain baseline $\jbase$.\\
We can formalize it as: 
\begin{align}\label{con:6}
\frac{1}{K}\sum_{k=1}^{K}J(\vtheta^{k(t)}) \geq \jbase,
\end{align}
where $\vtheta^{k(t)}$ denotes one of $K$ policies that are evaluated within iteration $t$. This is weaker than type-I, since we have no guarantee on the performance of the single policies that are evaluated. (some of them may perform badly, as long as others are able to compensate). The $K$ policies may represent, \eg intermediate updates to subsets of parameters (as in Section \ref{sec:theory}) or multiple updates within a learning iteration. 

Even more interestingly, we use constraint (\ref{con:6}) to model the following scenario: assume we want to optimize a target policy $\pi_{\vtarget[]}$ while running a (possibly different) learning policy $\pi_{\vtheta}$. For instance, we may want to find an optimal deterministic controller, while employing a stochastic policy to explore original solutions (as in Deterministic PG algorithms \cite{silver2014deterministic}). \\
In this case we can restate \Cref{con:6} in this way:
\begin{align}\label{con:6target}
(J(\vtheta^t) + J(\vtarget))/2 \geq \jbase
\end{align}

\begin{note}
The bound in \Cref{con:6} can be obtained by setting $\vtarget[] \equiv \vtheta$. In this case we are optimizing the performance of the learning policy $\pi_{\vtheta}$ and we are not evaluating any target policy.
\end{note}

Note that evaluating the target policy is not mandatory, but may provide useful information on the quality of the current solution. \\
Again, this is equivalent to constraining each parameter update:
\begin{equation}\label{con:type2}
J(\vtheta^{t+1}) - J(\vtheta^{t}) + J(\vtarget[t+1]) - J(\vtarget) \geq C^t,
\end{equation}
for some $C^t\in \realspace$.
%
In the spirit of type-II safety, we define a quantity $B$, called \textit{budget}, defined as follows:
\begin{definition}[Budget]
\begin{equation}
B^t =  \sum_{k=0}^{t} J(\vtheta^t) - J(\vtheta^{t-1}) + J(\vtarget) - J(\vtarget[t-1]).
\label{eq:budget}
\end{equation}
\end{definition}
This quantity has an intuitive economical meaning: it includes all the earnings that we got so far through our policy optimization process.\\ 
When $B^t > 0$ it means that our current policy performs in average better than the baseline, so we can potentially yield a worse policy without breaking the constraint (bounded worsening)\\
When $B^t<0$, instead, it means that the current policy is in average worse than the baseline, so we are required to take safer updates and improve the performance (required improvement). 

\begin{note}
In general we want the budget to be non-negative. However approximation errors and stochasticity of the environment may yield noisy estimates that could bring to situations where the budget is negative. If this is the case, the intended behaviour would be to recover the budget as soon as possible.
\end{note}

Type-II safety can be characterized as a budget constraint:
\begin{restatable}{theorem}{nonnegativebudget}\label{th:nonnegativebudget}
Any iterative learning algorithm that maintains $B^t \geq 0$ fulfills type-II safety with respect to a constant baseline $\jbase[] = [J(\vtheta^0) + J(\vtarget[0])] / 2$.
\end{restatable}
%
We identify three special cases of type-II safety corresponding to different choices of target policy:

\textbf{Lower-Bounded II (LB-II)}: When $\vtarget[]=\vtheta$, we aim to maximize the online performance without doing worse than the initial policy, in terms of average iteration performance. This is a common online-learning scenario.

\textbf{Low-Variance (LV)}:  Here $\vtarget[]$ is stochastic, but less exploratory than $\vtheta$. An example scenario is the case in which learning is performed on a prototype before testing on the real system, which may be more critical. 

\textbf{Zero-Variance (ZV)}: Here $\vtarget[]$ is deterministic, just like in the above mentioned deterministic controller example. Unfortunately, as we will see in the next section, it is more challenging to give guarantees on deterministic policies.  

\begin{samepage}
\begin{table}
	\caption{Overview of the safety requirements outlined in Section \ref{sec:framework}.}\label{tab:algorithms}
	\resizebox{\linewidth}{!}{
	\begin{tabular}[t]{  l | c c c c  }
		Name &  Target policy $\vtarget$ & Baseline $\jbase$ & $C^t$ & Type \\
		\toprule
		Monotonic Improvement (MI) &  $\vtheta^t$ & $J(\vtheta^{t})$ & $ 0$ & I\\
		
		Lower-Bounded I (LB-I) &  $ \vtheta^t$ &$J(\vtheta^0)$ & $J(\vtheta^0) - J(\vtheta^t)$ &  I  \\
		
		Lower-Bounded II (LB-II) &  $ \vtheta^{t}$ &$J(\vtheta^0)$ & $-B^t$ &  II \\
		
		Low-Variance (LV)  &  $\tilde{\sigma}^t<\sigma^t$ &$[J(\vtheta^0) + J(\vtarget[0])]/2$ & $-B^t $ &  II  \\
		
		Zero-Variance (ZV) &  $\tilde{\sigma}^t = 0$ &$[J(\vtheta^0) + J(\vtarget[0])]/2$ & $-B^t$ & II
	\end{tabular}
}
\end{table}

The safety requirements that we have identified are summarized in Table \ref{tab:algorithms}. In 
Section \ref{sec:algos} we propose a general algorithm that can be customized to match any of these constraints.
\end{samepage}

\section{Theoretical Analysis}\label{sec:theory}
In this section we will develop new theoretical results that will be the main building blocks of the SEPG algorithm outlined in \ref{sec:algos}. In particular, we will first extend the results obtained by Pirotta et al.~\cite{adaptive_step} explained in in~\Cref{sec:ass} to deal with an adaptive exploration parameter $\sigma$. Next, we will derive a new way to update $\sigma$ to allow for exploration. 

\subsection{Preliminaries}

In this theoretical analysis we will focus on the monotonic improvement case where $\vtarget=\vtheta^t$, so that the safety constraint can be expressed as:
%
\begin{align*}
J(\vtheta^{t+1}) - J(\vtheta^t) \geq C^t, \quad C^t \in \mathbb{R}, C^t < C_{\text{max}}.
\end{align*}
Here $C_\text{max}$ represents the maximum guaranteed improvement allowed by the theoretical bound on performance improvement. \\
From this specific case, we will see how to generalize to all the other cases presented in \Cref{sec:framework}.


Moreover, we will focus on Gaussian policies of the form:
\[
	\pi_{\vtheta}(a\vert s) = \frac{1}{\sqrt{2\pi}\sigma_{\vtheta}}\exp\left\{-\frac{1}{2}\left(\frac{a - \mu_{\vtheta}(s)}{\sigma_{\vtheta}}\right)^2\right\}.
\]
using the following, common parametrization:
\begin{align}\label{eq:parametrization}
\mu(s)=\vv^T\vphi(s), \qquad \sigma=e^w, \qquad \vtheta=[\vv\vert w],
\end{align}
where $\vphi$ are bounded state-features, \ie $\phi_i(s) \leq M_{\vphi}$ for each $s\in\Sspace, i=1,\dots,m$.
\begin{note}
This parametrization is such that the we have always a positive exploration coefficient $\sigma=e^w$ and the deterministic policy is approached when $w \rightarrow -\infty$. For this reason, we will define the deterministic policy with $\pi_{\vtheta}$ where $\vtheta = \transpose{[\vv \mid -\infty]}$
\end{note}

More complex representations are possible, \eg $\mu$ can be parametrized by a neural network. In some cases, also the standard deviation can be state-dependent. However in this work we will only consider linear-mean parametrization. \\
Another common generalization is to extend the policy to multi-dimensional action spaces by employing an independent Gaussian policy for each action dimension. This extension will be discussed in \Cref{app:multi}.

Given this new notation, we can restate the theorems described in \Cref{sec:ass} and \Cref{sec:abs}:
\begin{restatable}[From Theorem 3.3 in~\cite{adaptive_batch}]{theorem}{safetheta}\label{th:safetheta}
	Assuming $w^t = \text{const} $ for all $t$, update rule (\ref{up:onlyv}) guarantees:
	\begin{align}\label{stat:1_1}
	J(\vv^{t+1}, w^t) - J(\vv^t, w^t) \geq \alpha\norm[\infty]{\gradJ[\vv]{\vv^t,w^t}}^2 - 
	\alpha^2 c \norm[\infty]{\gradJ[\vv]{\vv^t,w^t}}^2,
	\end{align}
	where $c = \frac{RM_{\phi}^2}{(1-\gamma)^2\sigma^2}\left(\frac{|\mathcal{A}|}{\sqrt{2\pi}\sigma} +	\frac{\gamma}{2(1-\gamma)}\right)$ and  $|\mathcal{A}|$ is the volume of the action space. Guaranteed improvement is maximized by using a step size $\alpha^* = \frac{1}{2c}$, yielding:
	\begin{align}\label{stat:1_2}
	J(\vv^{t+1},w^t) - J(\vv^t,w^t) \geq \frac{\norm[\infty]{\gradJ{\vv^t,w^t}}^2}{4c}
	\coloneqq C_{\text{max}}^{\vv}.
	\end{align}
	Moreover, for any $C\leq C_{\text{max}}^{\vv}$, the following constraints on the step-size $\alpha$:
	\begin{align}\label{stat:1_3}
	|\alpha - \alpha^*| \leq \lambda_{\vv}\alpha^*, 
	\end{align}
	where $\lambda_{\vv}=\sqrt{1 - \nicefrac{C}{C_{\text{max}}^{\vv}}}$, are enough to guarantee $J(\vv^{t+1},w^t) - J(\vv^t,w^t)\geq C$.
\end{restatable}
This will be the starting point for the adaptive variance case described in the next section.\\
For simplicity, we assume to be able to compute exact policy gradients and we focus on one-dimensional action spaces. We deal with approximation errors in \Cref{sec:approx} and multi-dimensional actions in \Cref{app:multi}.

\subsection{Safe PG with adaptive variance}\label{sec:safe+explore}
As we have seen in \Cref{ch:safepg}, a way to guarantee performance improvement in policy gradient is to adaptively tune the step size $\alpha$ of the parameter updates to avoid large updates. For the case of Gaussian policies with linear mean and fixed variance, Papini et al.~\cite{adaptive_batch} propose a greedy-coordinate-ascent update:
\begin{align}\label{up:onlyv}
v_{k}^{t+1} \gets \alpha\gradJ[v_{k}]{\vv^t,w^t}
\quad \text{if}\:k =  \argmax_i |\gradJ[v_i]{\vv^t, w^t}| \:\text{else}\:v_{k}^t,
\end{align}
proving that it yields better guarantees than any gradient ascent update.\\
Only the parameter with maximum absolute-value partial derivative is updated, and ties can be broken in any way. Greedy coordinate ascent does not inherit the convergence properties of coordinate ascent, but has been proven to converge anyway~\cite{nutinicoordinate}. \Cref{th:safetheta} shows the performance improvement properties of this algorithm.


We generalize this result to the adaptive-variance case. Since the above guarantees require a fixed $w$, we can split the update rule in two parts: we first update the mean parameter ($\vv$-update) keeping $w$ fixed, then we update the variance parameter ($w$-update) keeping the mean parameters $\vv$ fixed:
%
\begin{align}\label{eq:naive_update}
\begin{cases}
v_{k}^{t+1} \gets v_{k}^t + \alpha\gradJ[v_{k}]{\vv^t,w^t}
	& \text{if}\:k =  \argmax_i |\gradJ[v_i]{\vv^t, w^t}| \:\text{else}\:v_{k}^t\\
w^{t+1} \gets w^t + \beta\gradJ[w]{\vv^{t+1}, w^t},
\end{cases}
\end{align}
%
where $\beta$ is a step size for the variance parameter.
Since, in general, the intermediate policy $\pi(\cdot\vert\vv^{t+1},w^t)$ must be evaluated in order to compute $\gradJ[w]{\vv^{t+1},w^t}$, we actually need to divide the required improvement $C$ between the two updates:
%
\begin{align}%\label{con:double}
&J(\vv^{t+1},w^t) - J(\vv^t,w^t) \geq (1-\nu) C \quad \text{and}\quad \label{eq:update-first} \\ 
&J(\vv^{t+1},w^{t+1}) - J(\vv^{t+1},w^t) \geq \nu C, \label{eq:update-second}
\end{align}
with any coefficient $\nu \in [0,1]$.

The first constraint (\ref{eq:update-first}) is already covered by Theorem \ref{th:safetheta}, by replacing $C$ with $(1-\nu)C$, since $\sigma$ stays constant during the $\vv$-update. \\
Addressing the problem of establishing similar performance improvement properties for the $w$-update in (\ref{eq:update-second}), we developed the following result:
\begin{restatable}[]{theorem}{safesigma}\label{th:safesigma}
	Assuming $o(|\Delta w|^3)$ terms can be neglected,\footnote{This approximation is not critical since the steps produced by safe algorithms tend to be very small.} the $w$-update in (\ref{eq:naive_update}) guarantees:
	\begin{align}\label{stat:2_1}
	J(\vv^{t+1}, w^{t+1}) - J(\vv^{t+1}, w^t) \geq \beta\gradJ[w]{\vv^{t+1}, w^t}^2 - \beta^2 d\gradJ[w]{\vv^{t+1}, w^t}^2,
	\end{align}
	where $d=\frac{R}{(1-\gamma)^2}\left(\frac{\psi|\Aspace|}{2\sigma_{w}}+\frac{\gamma}{1-\gamma}\right)$ and $\psi = \nicefrac{4(\sqrt{7} - 2)\exp(\nicefrac{\sqrt{7}}{2} - 2)}{\sqrt{2\pi}} \simeq 0.524$. Guaranteed improvement is maximized by using a step size $\beta^*=\frac{1}{2d}$, yielding:
	\begin{align}\label{stat:2_2}
	J(\vv^{t+1},w^{t+1}) - J(\vv^{t+1}, w^t) \geq \frac{\gradJ[w]{\vv^{t+1}, w^t}^2}{4d}
	\coloneqq C_{\text{max}}^w. 
	\end{align}
	Moreover, for any $\nu C\leq C_{\text{max}}^{w}$, the following constraints on the step-size $\beta$:
	\begin{align}\label{stat:2_3}
	|\beta - \beta^*| \leq \lambda_{w}\beta^*, 
	\end{align}
	where $\lambda_{w}=\sqrt{1 - \nicefrac{\nu C}{C_{\text{max}}^{w}}}$, are enough to guarantee $J(\vv^{t+1},w^{t+1}) - J(\vv^{t+1},w^t)\geq \nu C$.
\end{restatable}

Given this result, we can safely adapt the mean and the variance of a Gaussian policy by employing a sequence of $\vv$-update (\ref{eq:update-first}) and $w$-update (\ref{eq:update-second}) bounded by the results provided by \Cref{th:safetheta} and \Cref{th:safesigma} respectively. However, as stated in \Cref{sec:sepg-intro}, a greedy update of $w$ may not be a good choice because of the high penalties given by the exploration, that would quickly turn exploration to low values. The next section will provide a solution for this problem.

\subsection{Safely Exploring PG}\label{sec:explore+safe}
The $w$-update in (\ref{eq:naive_update}) does not take into consideration, in any way, the potential benefits of increasing exploration. In practice, following $\nabla_{w}J$ often results in greedy learning, setting the policy variance to small values very soon in order to exploit the current solution, which is usually suboptimal. More far-sighted algorithms would keep the variance high, or even increase it, long enough to properly explore the space of policies. \\
We now provide a variant of (\ref{eq:naive_update}) that is more exploratory while preserving safety guarantees. The first key intuition is that, in many situations, larger values of $\sigma_{w}$ can positively affect the guaranteed improvement yielded by the next $\vv$-update. Hence, we want to update $w$ in order to maximize this improvement. \\
We first need a simplified version of Theorem \ref{th:safetheta}:
\begin{restatable}[]{corollary}{simplev}\label{th:simplev}
	The $\vv$-update in (\ref{eq:naive_update}) with optimal step size $\alpha^*$ from Theorem \ref{th:safetheta} guarantees:
	\begin{align*}
	J(\vv^{t+1},w^t) - J(\vv^t,w^t) \geq \frac{\norm[2]{\gradJ[\vv]{\vv^t,w^t}}^2}{4mc}
	\coloneqq \Deltav(\vv^t,w^t),
	\end{align*}
\end{restatable}
where $\Deltav$ is a looser, but differentiable, lower bound on performance improvement for the $\vv$-update. Note that we the step size $\alpha$ used to update $\vv$ is not relevant, so we used the optimal step size $\alpha^*$ provided in~\cite{adaptive_batch} instead of the one used for the actual update. \\
We then use $\Deltav$ as a surrogate objective for the $w$-update:
%
\begin{align}\label{up:exp}
\begin{cases}
v_{k}^{t+1} \gets v_{k}^t + \alpha\gradJ[v_{k}]{\vv^t,w^t}
& \text{if}\:k =  \argmax_i |\gradJ[v_i]{\vv^t, w^t}| \:\text{else}\:v_{k}^t\\
w^{t+1} \gets w^t + \beta\gradDelta(\vv^{t+1}, w^t).
\end{cases}
\end{align}
%
The idea is to update $w$ as to maximize the performance improvement that the subsequent $\vv$-update is able to guarantee. This is a more far-sighted version of (\ref{up:exp}), using the bound in Corollary \ref{th:simplev} to look one step ahead. Since high values of $w$ lead to better performance improvement bounds, this typically favours exploration over exploitation.\\
It remains to establish the safety of this new update. 
The next key insight is that Theorem \ref{th:safesigma} provides a measure of guaranteed improvement for any step size $\beta$, even negative ones. So, even if the $w$-update is in the direction of $\gradDelta$, we can measure guaranteed improvement by the corresponding step in the direction of $\nabla_{w}J$ with a simple rescaling:
\begin{restatable}[]{theorem}{allsafe}\label{th:safe_exp}
	The $w$-update in (\ref{up:exp}) guarantees, for any (possibly negative) step size $\beta$:
	\begin{align*}
	J(\vv^{t+1},w^{t+1}) - J(\vv^{t+1},w^t) \geq \beta\gradDelta(\vv^{t+1},w^t)\gradJ[w]{\vv^{t+1},w^t}-
	\beta^2 d \gradDelta(\vv^{t+1},w^t)^2.
	\end{align*}
	Guaranteed improvement is maximized by using a step size $\tilde{\beta} =\frac{\gradJ[w]{\vv^{t+1},w^t}}{2d\gradDelta(\vv^{t+1},w^t)}$, yielding:
	\begin{align*}
	J(\vv^{t+1},w^{t+1}) - J(\vv^{t+1}, w^t) \geq C_{\text{max}}^w. 
	\end{align*}
	Moreover, for any $\nu C\leq C_{\text{max}}^{w}$, the following constraints on the step-size $\beta$:
	\begin{align}\label{stat:4_3}
	|\beta - \tilde{\beta}| \leq \lambda_{w}|\tilde{\beta}|
	\end{align}
	are enough to guarantee $J(\vv^{t+1},w^{t+1}) - J(\vv^{t+1},w^t)\geq \nu C$.
\end{restatable}
%
Finally, to further encourage parameter-space exploration, we propose to use the largest step sizes that satisfy the safety guarantees. This is natural for the $w$-update, that is ancillary to the following $\vv$-update. As for the $\vv$-update, a larger step size can substantially improve convergence speed. In fact, an improvement in the $\vv$-update can obtain some budget to be spent on exploration in the subsequent $w$-update. A more efficient exploration can yield a more accurate estimation for the next $\vv$-update and so on, ending in a positive feedback loop. \\
From \Cref{th:safetheta} and \Cref{th:safe_exp}, respectively, the largest safe $\vv$-step size is $\overline{\alpha}\coloneqq(1+\lambda_{\vv})\alpha^*$, and the largest safe $w$-step size is $\overline{\beta}\coloneqq\tilde{\beta} + \lambda_{w}|\tilde{\beta}|$. Hence, our proposed update rule is:
%
\begin{align}\label{up:morexp}
\begin{cases}
v_{k}^{t+1} \gets v_{k}^t + \overline{\alpha}\gradJ[v_{k}]{\vv^t,w^t}
& \text{if}\:k =  \argmax_i |\gradJ[v_i]{\vv^t, w^t}| \:\text{else}\:v_{k}^t\\
w^{t+1} \gets w^t + \overline{\beta}\gradDelta(\vv^{t+1}, w^t),
\end{cases}
\end{align}

which combines greediness and exploration while satisfying Safety Constraint~(\ref{eq:update-first}, \ref{eq:update-second}) for any ${C\leq\min\{C_{\text{max}}^{\vv},C_{\text{max}}^{w}\}}$ and $\nu\in[0,1]$. 

\begin{figure}
\includegraphics[width=\textwidth]{pictures/image_parabola}
\caption{The step size $\overline{\beta}$ in (\ref{up:morexp}) can be in the direction of $\gradJ[w]{\vv, w}$, in which case it will guarantee an improvement, or can be in the opposite direction (in the direction of $\gradDelta(\vv, w)$) when $C$ is negative, guaranteeing a bounded worsening. }
\label{fig:boobs}
\end{figure}

\Cref{fig:boobs} recaps the idea of this novel update. By considering an update rule for the $w$-update of this form: $w^{t+1} \gets w^t + \overline{\beta} \nabla_w J_\mu(\vv^{t+1}, w^t)$ we can have two cases:
\begin{itemize}
\item $\nabla_w J(\cdot) \cdot \nabla_w \Deltav(\cdot) > 0$: The update direction of the surrogate objective is the same as the na\`ive one. In this case the step size $\overline{\beta}$ will be positive and will follow both the surrogate and the na`ive gradient.
\item $\nabla_w J(\cdot) \cdot \nabla_w \Deltav(\cdot) < 0$: In this case we are likely to move in the opposite way w.r.t. the na\`ive gradient $\nabla_w J_\mu(\cdot)$. More specifically, we can identify two sub-cases:
\begin{itemize}
\item If we must guarantee a required improvement, then the step size $\overline{\beta}$ will be negative.
\item Otherwise, if we can afford a bounded worsening, the step size $\overline{\beta}$ will be positive.
\end{itemize}
\end{itemize}

\subsection{Remarks on gradient approximation}\label{sec:approx_short}
When the true policy gradient cannot be computed, we can use the REINFORCE estimator (\ref{eq:reinforce}). The estimation of $\nabla_w\mathcal{L}$ is more involved, but we provide an unbiased estimator in Appendix \ref{app:estimating}. In this \textit{approximate framework}, the performance improvement bounds of this section admit high-probability variants. Using these bounds require to characterize gradient estimation errors, \eg by using distribution-independent statistical inequalities, as done in \cite{adaptive_batch}. In this work, we choose to employ t-based confidence intervals, making a simplifying assumption on the trajectory-generating distribution\footnote{Being the bounds very conservative, this is not critical. A similar assumption is done in \cite{pmlr-v37-thomas15}.}. More details can be found in Appendix \ref{sec:approx}.


\section{Algorithms}\label{sec:algos}

In this section we provide SEPG (Safely Exploring Policy Gradient), a general algorithm that can be customized to match all the safety requirements in Table \ref{tab:algorithms}. We use the same acronyms to denote both safety requirements and the corresponding SEPG variants. Algorithm \ref{alg:adaptive_exp} shows the general pseudocode that, as is, guarantees type-II safety with a target policy. The algorithm keeps a budget $B$. The initial-budget parameter $B^0$ is useful for tasks that require a lot of exploration and represents the initial cost that we are willing to pay to be able to solve the task, but can be set to $0$ in most cases. The code can be divided in three parts: P1 performs a $\vv$-update with step size $\overline{\alpha}$; P2 evaluates the target policy (only if necessary); P3 performs a $w$-update with step size $\overline{\beta}$. These three parts can be reordered in any way without breaking safety guarantees.
%
%We propose this particular ordering since, this way, the $w$-update has a higher available budget on average. 
%
The budget is updated with the measured performance improvement each time a policy is evaluated. The batch of $N$ trajectories available at each iteration is equally split among all policy evaluations.
The first difference between SEPG variants is the target policy.
For LV, we use a low-variance policy $\vtarget = [\vv^t \vert\wtarget]^T$ with $\wtarget<w^t$. For ZV, a deterministic policy with $\vtarget=[\vv^t\vert-\infty]^T$. For all the other variants, $\vtarget=\vtheta^t$. To obtain LB-I we also need to set $B^t = J(\vtheta^t) - \jbase$ in lines~\ref{alg:computealpha} and~\ref{alg:computebeta}. To obtain MI, we set $B^t = 0$ everywhere instead.
Algorithm \ref{alg:adaptive_exp} preserves the invariant $B^t\geq 0$. This can be shown straightforwardly for P1 and P3 by setting $C=-B$ in equation (\ref{up:morexp}), while P2 requires some considerations: for ZV, we cannot use the bounds from theorem \ref{th:safetheta} anymore, hence the invariant is guaranteed only on an infinite number of iterations. To keep guarantees, we can use LV and set $\wtarget[]$ to the lowest value $\tilde{w}$ that guarantees a bounded worsening of at most $B$, using constraints (\ref{stat:2_3}) from Theorem \ref{th:safesigma}. 
From Theorem \ref{th:nonnegativebudget}, the invariant $B^t \geq 0$ guarantees type-II safety. The LB-II variant achieves type-I safety for the $\vv$-update, since:
\begin{equation*}
 J(\vv^{t} + \alpha \nabla_{\vv}J(\vv^t, w^t), w^t) - \cancel{J(\vv^t, w^t)} \geq \jbase[t] - \cancel{J(\vv^t, w^t)},
\end{equation*}
and for the $w$-update, since:
\begin{align*}
J(\vv^t+1, w^t + \overline{\beta} \gradDelta(\vv^{t+1}, w^t)) - \cancel{J(\vv^{t+1}, w^t)} &\geq \jbase[t] - \cancel{J(\vv^{t+1}, w^t)}
\end{align*}
Analogous considerations hold for MI.
\begin{algorithm}[t]
\caption{Safely-Exploring Policy Gradient (SEPG)}
    \label{alg:adaptive_exp}
    \begin{algorithmic}[1] 
	\State \textbf{input:} $\vtheta^0 = [\vv^0, w^0]$, $T$, $N$, $B^0$
        \State \textbf{initialize:} $B \gets B^0$
        
        \State \textbf{evaluate} $\jbase[] = \left(J(\vv^0, w^0) + J(\vtarget[0])\right) / 2$
        \For {t = 1 \ldots $T$}
            \State \textbf{evaluate} $J(\vv^t,w^t)$, $\nabla_{\vv}{J(\vv^t,w^t)}$ \label{alg:evaluate1}\Comment{P1}
			\State Compute largest $\overline{\alpha}$ that guarantees $J(\vv^t + \overline{\alpha} \nabla_{\vv}J(\vv^t, w^t), w^t) - J(\vv^t, w^t) \geq -B$ \label{alg:computealpha}
            \State $\vv^{t+1} \gets \vv^t + \overline{\alpha}\nabla_{\vv}{J(\vv^t, w^t)}$  \Comment{$\vv$-update}
            \State $B \gets B + J(\vv^{t+1}, w^t) - J(\vv^t, w^t)$.


            \If{$\vtarget \neq \vtheta^t$}\Comment{P2}
	            \State \textbf{evaluate} target policy $J(\vtarget)$ without losing more than $B$ budget units \label{alg:evaluatetarget}
    		        \State $B \gets B + J(\vtarget) - J(\vtarget[t-1])$
        		 \EndIf   
            
            \State \textbf{evaluate} $J(\vv^{t+1}, w^t)$, $\nabla_{w}{J(\vv^{t+1}, w^t)}, \gradDelta(\vv^{t+1},w^t)$ \label{alg:evaluate2}\Comment{P3}
            \State Compute largest $\overline{\beta}$ that guarantees $J(\vv^{t+1}, w^t + \overline{\beta} \gradDelta(\vv^{t+1}, w^t)) - J(\vv^{t+1}, w^t) \geq -B $ \label{alg:computebeta}
            \State $w^{t+1} \gets w^t + \overline{ \beta}\gradDelta(\vv^{t+1},w^t)$ \Comment{$w$-update}            
            \State $B \gets B + J(\vv^{t+1}, w^{t+1}) - J(\vv^{t+1}, w^t)$. 
        \EndFor
    \end{algorithmic}
\end{algorithm}


%\bibliography{nips_2018}
%\bibliographystyle{unsrt}
%
%\clearpage
%\appendix
