\chapter{Experimental analysis}
\label{ch:experim}
%\thispagestyle{empty}

\fancyhead[LE,RO]{\bfseries\thepage}                               
\fancyhead[RE]{\bfseries\leftmark}    
\fancyhead[LO]{\bfseries\rightmark}  

In \Cref{ch:balance} we have presented a new general framework for safe reinforcement learning that can be used to design safe constraints depending on the user needs. In particular we have divided the safety constraints in two families, that we called type-I and type-II.\\ Type-I constraints target more critical situations, where we need a strict safety guarantee. Type-II constraints, instead, target a more flexible situation, where we are only concerned on guarantees over a learning iteration. Within this framework, we identified several variants, that fulfil different needs in practical situations.
%In particular, we presented five possible variants of particular interest: \textbf{Monotonic Improvement} (MI), that gives the strictest guarantee and can be used in critical applications or when costs/risks are not modelled in the reward; \textbf{Lower Bounded I} (LB-I), that sets the performance of the initial policy as a lower bound for each subsequent update, and can be used when we already have a good initial policy but we want to improve it; \textbf{Lower Bounded II} (LB-II), that guarantees type-II safety over a learning iteration and can be used in an on-line scenario where we don't want to perform worse than the initial policy on average; \textbf{Low-Variance} (LV) that adds the possibility to test the quality of a target policy under low-variance conditions (\eg a prototype); \textbf{Zero Variance} (ZV) where we sacrifice some guarantees and we test a deterministic controller, which gives a more realistic evaluation of the target policy and possibly allows for a faster convergence speed.\\
Specifically, in \Cref{sec:algos}, we have described the Safely-Exploring Policy Gradient (SEPG) algorithm and how to adapt it to implement such variants.


In this chapter, we will evaluate all these variants on simulated continuous control tasks: in \Cref{sec:lqg}, we will consider the linear-quadratic Gaussian (LQG) control problem. This is a well-known control task for which a closed-form solution exists, which let us study in more details the behaviour of our algorithms. Then, in \Cref{sec:mountain}, we will consider the Mountain Car problem, which is a complex task that requires a particularly careful exploration strategy. 


\section{Linear Quadratic Gaussian Controller}\label{sec:lqg}

The linear-quadratic Gaussian (LQG) regulator problem~\cite{peters} is a continuous MDP defined by a transition model:
\[
s_{t+1}\sim \mathcal{N}(s_t + a_t, \eta_0^2),
\]
where the next state is obtained by summing the action and some noise to the current state, a Gaussian policy:
\[
a_t \sim \mathcal{N}(\transpose{\vv}s_t, e^{2w}),
\]
and a reward function that depends on the distance to the goal and the magnitude of the action:
\[
r_t = -0.5(s_t^2 + a_t^2).
\]
Both action and state space are continuous and bounded to the interval $\sspace, \aspace = [-2, 2]$ and the initial state $s_0$ is drawn uniformly at random from $\sspace$. The goal of this task is to bring the state $s_t$ to zero. \\
The main difficulty of this task is reflected in the reward function, that depends both on the state and the action. This poses a trade-off on the magnitude of the action: we want to reach the goal fast enough to avoid the penalization given by the distance to the goal, but slow enough to avoid a strong penalization on the action. 

This problem admits a closed-form solution for Gaussian policies with mean linear in the state space. In all our experiments, we use a discount factor of $\gamma=0.99$, which gives an optimal parameter $\vv^* \approx -0.615$ corresponding to an expected deterministic performance $J(\vv^*, -\infty)\approx-7.753$. We always start from an initial policy with $\vv^0=-0.1$. This is a rather good starting point, since it is far from the optimum   and values closer to 0 make the controller less stable. \\
The knowledge of the optimal policy $\vtheta^*$ gives us the possibility to compare the algorithms on how close they are w.r.t. the optimal solution.

We will gradually present the results in this way: we will compare Monotonic Improvement (MI) with Lower Bounded I (LB-I) first, which are the two implementations of type-I safety. Next we will also add the other variants and we will make some considerations about the difference between type-I and type-II safety. Finally, we will compare Zero Variance (ZV) with some alternative implementations that simultaneously perform a $\vv$-update and a $w$-update, thus requiring less trajectories, but providing no guarantees.

\subsection{Type-I safety constraints}

Here we want to focus on the comparison between MI and LB-I. They both guarantee a type-I safety constraint of the kind:
\[
J(\vtheta^{t+1}) \geq \jbase
\]
for each update. The main difference is the value of the baseline: MI sets the baseline to be the performance of the policy at the previous iteration, namely $\jbase[t] = J(\vtheta^t)$, while LB-I lower-bounds the performance of the exploratory policy w.r.t. the initial policy, that is, $\jbase = J(\vtheta^0)$, that guarantees $J(\vtheta^t) \geq J(\vtheta^0)$ for each update.\\
This adds a new degree of freedom that greatly increase the learning speed, as shown in \Cref{fig:plot1}. In particular, the advantage of LB-I is that it is not required to improve the solution on every step, but it can afford to lose up to $J(\vtheta^t) - J(\vtheta^0)$, that is, the advantage of the current policy w.r.t. the initial policy. This difference can be seen after some iterations of the algorithm, where LB-I's advantage over MI increases, allowing it to take even larger updates.\\
Moreover, LB-I safely increases the exploration parameter $\sigma$ (\Cref{fig:plot1-2}) by following the newly introduced quantity $\gradDelta$, that further increases the convergence speed of the parameter $\vv^t$, as shown in \Cref{fig:plot1-3}. The MI variant, instead, is not able to significantly change the variance $\sigma$ because of its high conservativeness. 

% PLOT 1
\begin{figure}[t]
\centering
\begin{subfigure}[t]{\textwidth}
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot1.pdf}
\caption{Performance of the exploratory policy \\Note that: $\vtheta^t_E = \vtheta^t_T$} \label{fig:plot1-1}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot2.pdf}
\caption{Evolution of the variance parameter $\sigma$} \label{fig:plot1-2}
\end{subfigure}
\end{subfigure}
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot3.pdf}
\caption{Evolution of the Gaussian mean parameter $\vv^t$} \label{fig:plot1-3}
\end{subfigure}
\caption[Comparison between MI and LB-I implementations.]{Comparison between MI and LB-I implementations. The less stringent guarantees given by LB-I allows for a faster convergence speed of the mean parameter $\vv^t$ and an increase in exploration. }
\label{fig:plot1}
\end{figure}

\subsection{Type-II safety constraints}
In this section, we will compare the behaviour of all type-II variants of SEPG, shown in \Cref{fig:plot2}. \\
Specifically, (a) shows the performance of the exploratory policies; (b) the evolution of $\sigma$ in the different variants; (c) the evolution of the mean parameter $\vv^t$; (d) the safety guarantee that is implemented; (e) the deterministic policy optimized by LV and ZV; (f) the evolution of the budget $B^t$. \\
We identify the following interesting behaviours:
\begin{itemize}
\item Contrary to type-I variants (MI, LB-I), that  tend to directly improve $J(\vtheta)$, type-II variants (LB-II, LV, ZV) invest the  performance improvements for later usage. Specifically, a positive performance improvement generates budget, that will be spent in exploration. The budget constitutes an upper-bound for the $\vv$-updates and $w$-updates, hence a high budget have the effect of decreasing the conservativeness of the algorithm by allowing bounded performance loss.

\item In \Cref{fig:plot2-4} we can see that the type-II safety guarantee:
\[
\frac{1}{K}\sum_k \left(J(\vtheta^{k(t)}) + J(\vtarget[k(t)])\right)/2 \geq \jbase
\]
is satisfied. We have highlighted the performance of the baseline policy with a red dashed line. \\
In particular note that LB-I, LV and ZV belong to different scales. This is due to the magnitude of the target policy: for LB-II, we do not evaluate the target policy (since we set $\vtheta^t_T = \vtheta^t_E$); for LV, we evaluate a target policy that is less stochastic than the exploratory policy, but it is still close to the exploratory policy; for ZV, we evaluate the deterministic policy, that has a higher performance than the low-variance policy. This can be seen in \Cref{fig:plot2-5}. \\
Moreover, we can see that they both end slightly below the baseline. This is due to two factors: the first is the gradient approximation, that adds some errors on our estimations. The second factor is related to the behaviour of the algorithm: as it is designed, the algorithm will try to spend all the available budget in bounded-worsening updates. This is fine as long as new income is added to the budget. When the parameter is close to the optimal value, though, it becomes hard to increase the budget. As it happens, the budget becomes slightly negative and the algorithm is not capable of restoring a positive value for it. 

\item The evolution of the online performance measure $J(\vtheta_E)$ is of particular interest: it grows to a high value at the beginning and then drops. This behaviour can be explained by looking at the evolution of $\sigma$ and $\vv^t$: as $\vv$ converges toward the optimal value, the performance of the policy increases and some of the gained budget is spent in raising $\sigma$. However, when $\vv$ is close to the optimum, there are no big improvements reflected in the performance measure, hence the budget is spent solely on exploration. This increases the exploration costs and explains the drop in the policy performance $J(\vtheta^t_E)$. Note that for ZV the drop is also related with the gain in the target policy.

\item The evaluation of the target policy for LV and ZV is shown in \Cref{fig:plot2-5}. The performance of the deterministic policy optimized by ZV ends very close to the optimum $J(\vv^*,-\infty)\approx-7.753$. The low-variance target policy optimized by LV, instead, is very similar to the exploratory policy $\vtheta^t_E$. This is because the bounds used to compute the low-variance policy do not allow for a significant change of the variance, which remains only slightly reduced. 
\end{itemize}

In general we can see that ZV converges faster to the optimal value $\vv^*$ than the other type-II variants. The main reason is that the evaluation of the deterministic policy increases the amount of available budget. The same behaviour can be seen for LV, but, since the low-variance policy still remain very close to the stochastic one, the improvement is much less than for ZV. \\
All these variants are notably faster than the type-I implementations described before.

% PLOT 2
\begin{figure}[t!]
\centering
\begin{subfigure}[t]{\textwidth}
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot4.pdf}
\caption{Performance of the exploratory policy} \label{fig:plot2-1}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot5.pdf}
\caption{Increase in the variance parameter $\sigma$} \label{fig:plot2-2}
\end{subfigure}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot6.pdf}
\caption{Convergence speed of the Gaussian mean parameter $\vv^t$} \label{fig:plot2-3}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot7.pdf}
\caption{The type-I and type-II safety guarantee. For MI,LB-I and LB-II we have $\vtheta^t_T = \vtheta^t_E$} \label{fig:plot2-4}
\end{subfigure}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot8.pdf}
\caption{Performance of the target policy for LV and ZV} \label{fig:plot2-5}
\end{subfigure}
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot13.pdf}
\caption{Evolution of the budget} \label{fig:plot2-6}
\end{subfigure}
\end{subfigure}

\caption[Comparison between SEPG type-II variants.]{Comparison between SEPG type-II variants presented in \Cref{sec:algos}. Depending on the user needs, the user can choose between different safe approaches. }
\label{fig:plot2}
\end{figure}

Next, we compare the convergence speed of these variants with other state-of-the-art non-safe approaches.
\Cref{exp:convergence-speed} shows a 95\%-confidence upper bound  on the absolute error $|\vv - \vv^*|$ after 10,000 iterations for different initial values of $\sigma$. We compare our algorithms to REINFORCE with constant step size ($\alpha = \beta = const$) and Adam~\cite{kingma2014adam}, an algorithm that starts from an initial learning rate, and subsequently adapts to the state of the optimization. \\
Our algorithms, of course, cannot outperform all possible combinations of manually-chosen step sizes, but we can see that our algorithms give comparable convergence speed without the need of tuning the step-size. This is an important aspect that allows SEPG to be more domain-independent.

In particular, we note that our algorithms perform better with higher initial values of $\sigma$. A possible explanation for this behaviour is that the bounds used to compute the optimal step size $\overline{\alpha}$ and $\overline{\beta}$ highly depend on $\sigma$: higher values of $\sigma$ make the bounds less tight, thus speeding up convergence speed.\\
Moreover, we can see that the Monotonic Improvement (MI) case is in general the slowest to converge due to the more conservative behaviour of the algorithm. This is the main reason that led the investigation of other types of safety constraints that better fit the user needs.


\begin{table}[t]

\resizebox{\linewidth}{!}{
%\setlength{\tabcolsep}{0.55em}
\begin{tabular}{rccccccccccc}
\toprule 
&\multicolumn{3}{c}{Adam} & \multicolumn{3}{c}{$\alpha = const$} &&&& \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7} 
$\sigma$ & 1e-4 & 1e-05 & 1e-06 & 1e-4 & 1e-5 & 1e-6 & MI & LB-I & LB-II & LV & ZV \\
\midrule
0.1 & 0.126 & 0.432 & 0.506 & 0.132 & \textbf{0.026} & 0.228 & 0.515 & 0 .510 & 0.368 & 0.429 & 0.435 \\
0.2 & 0.08 & 0.429 & 0.506 & 0.124 & \textbf{0.018} & 0.201 & 0.515 & 0.494 & 0.039 & 0.028 & 0.056 \\
0.5 & 0.061 & 0.428 & 0.506 & 0.122 & 0.012 & 0.125 & 0.511 & 0.29 & 0.009 & 0.01 & \textbf{0.006} \\
1.0 & 0.046 & 0.422 & 0.505 & 0.122 & 0.011 & 0.069 & 0.497 & \textbf{0.01} & 0.021 & 0.021 & 0.028 \\
\bottomrule
\end{tabular}
}
\caption[Upper bound confidence interval for LQG problem.]{Upper bound confidence intervals at 95\% of $|\vv - \vv^*|$ after 10,000 iterations.}\label{exp:convergence-speed}
\end{table}


\subsection{Alternative comparisons}

As we have introduced in the end of \Cref{sec:algos}, a possible way to further speed up the convergence speed of our algorithms is to reuse the collected trajectories and simultaneously perform a $\vv$-update and a $w$-update. This comes at the cost of not having strict guarantees that the algorithm will satisfy the safety constraint. We tested this variant, called SEPG-SIM, in the same LQG setting.

A problem that naturally arises when considering simultaneous updates is the allocation of the budget, that is, how to split the budget between the $\vv$-update and the $w$-update. We did not study this allocation issue in-depth, but we have only considered a fixed-strategy: at every iteration, the budget $B$ is split so that $\nu B$ is used for the $\vv$-update, and $(1-\nu)B$ is used for the $w$-update. Specifically, we tested values for $\nu \in \{\nicefrac{1}{3}, \nicefrac{1}{2}, \nicefrac{2}{3}\}$.\\
The results are shown in \Cref{fig:plot3}: as expected, the simultaneous approach required less samples to converge. Moreover, the safety constraint is still satisfied despite this approximation. Finally, we can see the difference between the different budget allocation strategies in \Cref{fig:plot3-3}: allocating more budget to the $w$-update (SIM-$\nicefrac{1}{3}$) gives a faster convergence for the parameter $\vv^t$. This aspect shows the strict relation between exploration and exploitation, where the former influences the latter in a non-trivial way.



\begin{figure}[t]
\centering
\begin{subfigure}[t]{\textwidth}
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot9.pdf}
\caption{Performance of the exploratory policy} \label{fig:plot3-1}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot11.pdf}
\caption{Evolution of the variance parameter $\sigma$} \label{fig:plot3-2}
\end{subfigure}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot10.pdf}
\caption{Evolution of the Gaussian mean parameter $\vv^t$} \label{fig:plot3-3}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{./pictures/plots/plot12.pdf}
\caption{The type-II safety guarantee. } \label{fig:plot3-4}
\end{subfigure}
\end{subfigure}

\caption[Comparison between ZV and SIM-$\nu$.]{Comparison between ZV and an alternative variant of SEPG (SIM-$\nu$) with simultaneous updates. The budget is split in this way: $\nu B$ for the $\vv$-update and $(1-\nu)B$ for the $w$-update. }
\label{fig:plot3}
\end{figure}



\section{Mountain Car}\label{sec:mountain}

We also tested our algorithms on the Mountain Car problem (in the \textit{openai/gym} implementation) to show the advantages of our algorithm in a situation where exploration is particularly important. 

We have introduced the problem of Mountain Car in \Cref{example:mc}. Briefly, a car is placed between two hills and the goal is to reach the top of the right hill by moving the car left or right. This problem is difficult for its exploration need, because the engine of the car is not powerful enough, hence the solution requires the agent to gain momentum by first going on the opposite direction. Moreover, the reward function includes a penalization on the magnitude of the action. This is where the problems begin: if the agent does not reach the goal soon enough, this penalization will push the agent to not move at all. This is a local maximum because the agent will get zero reward by standing still, while it will receive a negative reward if it moves but does not reach the goal. This penalty clearly outlines the role of exploration in a reinforcement learning task: we have to take actions that possibly yield bad immediate rewards, in order to obtain a good benefit in the future. This task, in particular, requires the agent to reach the goal at least once, before actually solving the problem.


We set $\sigma^0 = 0.2$, which is not enough to reach the goal. By providing a nominal initial budget ($B^0 = 1$), our method SEPG-ZV starts investing more and more in exploration until it reaches the goal, and is ultimately able to solve the problem. An important role is played by the choice of following the gradient $ \gradDelta(\vv^t, w^t)$ instead of the na\"ive gradient $\nabla_w J(\vv^t, w^t)$. \\
\Cref{fig:mc} shows the performance (on the left) and the policy variance (on the right) in our experiment. As we can see, a na\"ive gradient ascent update (Adam in the picture, but the same can be shown for any reasonable fixed step size), is not far-sighted enough to see any advantage in increasing $\sigma$, and gets stuck immediately. 


\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[width=\textwidth, height=0.8\textwidth]{./pictures/plot_MC3.pdf}
\caption{Performance of Adam and ZV algorithm.}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[width=\textwidth, height=0.8\textwidth]{pictures/plot_MC_sigma3.pdf}
\caption{Evolution of the exploration parameter $\sigma$}
\end{subfigure}
\caption{Mountain Car task, starting from a very low variance}\label{fig:mc}
\end{figure}





