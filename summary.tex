\chapter*{Abstract}
%\thispagestyle{empty}
\addcontentsline{toc}{chapter}{Abstract}

Safe Reinforcement Learning is necessary for critical applications, such as robotics, where exploratory behaviours can harm systems and people, but it also lends itself to economic interpretations. However, safe algorithms often tend to be overly conservative, sacrificing too much in terms of speed and exploration. The latter, in particular, is of fundamental importance for a learning algorithm to gain informations about the environment. In this thesis, we will investigate the non-trivial tradeoff between these two competing aspects, safety and exploration. 
Starting from the idea that a practical algorithm should be safe as needed, but no more, we identify interesting application scenarios and propose Safely-Exploring Policy Gradient (SEPG), a very general policy gadient framework that can be customized to match particular safety constraints. To do so, we generalize existing bounds on performance improvement for Gaussian policies to the adaptive-variance case and propose policy updates that are both safe and exploratory. 
