\chapter*{Abstract}
%\thispagestyle{empty}
\addcontentsline{toc}{chapter}{Abstract}

%In the context of Reinforcement Learning (RL), there are several control tasks that can be solved by learning algorithms, such as policy gradient methods, but often yield 

Reinforcement Learning is a powerful framework that can be used to solve complex control tasks. Among the current challenges that reinforcement learning has to face, there are the inner difficulties of exploring the environment and doing it safely.
Safe Reinforcement Learning is necessary for critical applications, such as robotics, where exploratory behaviours can harm systems and people, but it also lends itself to economic interpretations. However, safe algorithms often tend to be overly conservative, sacrificing too much in terms of speed and exploration. The latter, in particular, is of fundamental importance for a learning algorithm to gain informations about the environment. In this thesis, we will investigate the non-trivial tradeoff between these two competing aspects, safety and exploration. 
Starting from the idea that a practical algorithm should be safe as needed, but no more, we identify interesting application scenarios and propose Safely-Exploring Policy Gradient (SEPG), a very general policy gradient framework that can be customized to match particular safety constraints. To do so, we generalize existing bounds on performance improvement for Gaussian policies to the adaptive-variance case and propose policy updates that are both safe and exploratory. 
