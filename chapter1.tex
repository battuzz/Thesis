\chapter{Introduction}
\label{ch:intro}
\thispagestyle{empty}
\pagenumbering{arabic}
\setcounter{page}{1}


Every day we are required to make some choices: we choose what to wear every day, if it is more convenient to take the bus than the metro, whether to put the alarm for the next day and so on. We, as decision-makers, have learned how to tackle with these problems since we were children. The process that brought us to acquire this ability is long and complex, but it can be broken down to few essential pieces: (1) we have a perception of what is surrounding us, (2) we can interact with the environment by taking some actions and (3) the environment reacts by giving us a new perception. These three factors always come into play, whether we want to learn a new skill (\eg to play the piano), we want to play a game and so on. This interaction with the environment builds what we call experience. However, experience alone is not enough: we also have to identify a goal. In a very simple setting, we can say that we are happy if the accomplish the goal, otherwise we are sad. The amount of happiness (or sadness) can be seen as a reward that the environment gives us. What we naturally learn to do is to develop strategies that increase the amount of happiness obtained by our interaction with the environment. This process of learning, when applied to computer programs, takes the name of reinforcement learning.



Reinforcement Learning (RL) is a field of Machine Learning that closely resembles how living beings learn in nature. In this work we will explore the computational approach to learning from interaction, by considering a simplified yet powerful mathematical model. The final result will be a function that maps states to actions, that tells us what to do, so as to maximize a reward signal. This reward signal is intended to give insights about \textit{what} is the goal to pursue but not \textit{how}. The learner is not told which actions to take, but rather it has to discover which actions are more valuable via trial-and-error, from experience. \\
This is not a trivial task, since actions can potentially have long-term effects. As a matter of facts, an action affects the following state, from where another action can lead to another state and so on, generating a sequence of causally related events. Delayed rewards and trial-and-error search are the two most important aspects of reinforcement learning, that make this problem both difficult and intriguing.

Among the current challenges that reinforcement learning has to take, there are the inner difficulties of exploring the environment and to do it safely. Exploration is a fundamental aspect of learning, as it requires creativity and practice to effectively solve a difficult task. In this context, exploration is intended as taking actions that are different than the ones we are used to take. Slightly changing the way we do something can help you gain more knowledge about the environment, and potentially can help find new solutions. However, an excess in exploration is not always useful and can harm systems and people, besides having an intuitive economical impact. As an example, we can think of a robotic arm that is learning how to hit a ball with a baseball bat. It can happen that, during the learning phase, the robotic arm swings the bat with sudden movements, that could potentially break the system. This problem is also reflected in the costs of repairing the robotic arm.

In this thesis we want to combine these two competing aspects of safety and exploration, and devise a general solution that can be customized to the user needs. We will focus on a particular class of algorithms in the reinforcement learning literature called policy gradient methods. Besides having nice convergence properties, these methods are particularly suited for control tasks, as they are able to address many of the difficulties that arise in this field, such as high dimensional state spaces and noisy sensors. Moreover, they allow to easily incorporate prior domain knowledge in order to design safer and more effective results. Recent work in safe reinforcement learning have brought to nice theoretical results, but do not consider the exploration factor. In particular we will refer to the work of Kakade and Langford~\cite{Kakade02approximatelyoptimal} on safe policy iteration, later refined by Pirotta et al.~\cite{safe_iteration} and adapted to the policy gradient method in~\cite{adaptive_step}. Moreover, the results presented in this field mostly consider the restricted case of monotonic improvements, where the main goal is to improve the current solution at every iteration. This constraint has a big impact on the learning speed of the algorithm, since we need very small updates to safely improve the current solution. For this reason, these methods are known to be overly conservative, as they sacrifice too much in terms of speed and exploration. 

Starting from these results, we will introduce a new framework for safe reinforcement learning. This framework goes beyond the single monotonic improvement case, and it will include several kind of constraints, each corresponding to a different practical need. The user can then choose the type of safety constraint that better suits his requirements, without guaranteeing more that it is needed. Within this framework, we will extend the results from~\cite{adaptive_step} to also deal with adaptive exploration. The final contribution that we introduce in this work is a general algorithm named Safely-Exploring Policy Gradient (SEPG), that combines the new results developed in this thesis and can be customized to match particular safety constraints. 

The contents of the thesis are organized as follows: in \Cref{ch:rl} we will provide an overview of reinforcement learning and we will introduce the basic concepts that are needed to define policy gradient methods. In particular we will define a mathematical model called Markov Decision Process that describes the interactions between the agent and the environment. In \Cref{ch:safepg} we will describe in details the main results of safe reinforcement learning. Specifically, we will start from the results of Kakade and Langford~\cite{Kakade02approximatelyoptimal} on safe policy iteration that were later extended by~\cite{safe_iteration}. Pirotta et al.~\cite{adaptive_step} adapted the results to policy gradient, that were later extended by Papini et al.~\cite{adaptive_batch} to also include the batch size. In \Cref{ch:balance} we will introduce the main contribution of this thesis. We will first introduce the new safe reinforcement learning framework where we identify two main classes of safety  constraints. Then, we will derive new results for the adaptive exploration case and finally we will see how to include these results in a practical algorithm. We will describe in details how to adapt SEPG to every safety constraint. Next, in \Cref{ch:experim} we will evaluate SEPG variants on simulated continuous control tasks. We will also compare SEPG with other non-safe approaches and we will analyse the results. Finally, in \Cref{ch:conclusion} we will derive some conclusions and we will describe possible additions that can be done to extend this work.
 



%Reinforcement Learning is a field of machine learning that closely resembles how living entities learn in nature. We are able to learn by observing how the environment reacts to our actions. With a careful model designed to describe this interaction, we are able to apply this concept computationally. 
%
%The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct connection to its environment. Exercising this connection produces a wealth of information about cause and effect, about the consequences of actions, and about what to do in order to achieve goals. Throughout our lives, such interactions are undoubtedly a major source
%of knowledge about our environment and ourselves. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior. Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence.

