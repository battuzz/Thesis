\chapter{Conclusions}
\label{ch:conclusion}
%\thispagestyle{empty}

In this thesis, we have introduced a new framework and new techniques for safe reinforcement learning that combines some of the results done in this field. We identified two families of safety constraints: type-I safety, for the most critical situations where we must have guarantees on every policy update; type-II safety, for less critical situations where the safety constraint can be guaranteed over a \textit{learning iteration}. Within these families, we have also characterized several situations that correspond to practical user needs, providing a label for each one of them:
\textbf{Monotonic Improvement} (MI), that gives the strictest guarantee and can be used in critical applications or when costs/risks are not modelled in the reward; \textbf{Lower Bounded I} (LB-I), that sets the performance of the initial policy as a lower bound for each subsequent update, and can be used when we already have a good initial policy but we want to improve it; \textbf{Lower Bounded II} (LB-II), that guarantees type-II safety over a learning iteration and can be used in an on-line scenario where we do not want to perform worse than the initial policy on average; \textbf{Low-Variance} (LV) that adds the possibility to test the quality of a target policy under low-variance conditions (\eg a prototype); \textbf{Zero Variance} (ZV) where we sacrifice some guarantees and we test a deterministic controller, which gives a more realistic evaluation of the target policy and possibly allows for a faster convergence speed.



We then extended the results on performance improvement bounds from \cite{adaptive_batch,adaptive_step} on policy gradient with Gaussian policies to include an exploratory behaviour. We introduced a new quantity $\gradDelta(\vv^t, w^t)$ that was used as an alternative approach of $\nabla_w J(\vv^t, w^t)$ to handle exploration. This new approach was able to tackle a difficult problem such as Mountain Car.

Finally, we provided Safely-Exploring Policy Gradient (SEPG), a general algorithm that can be customized to every safety constraint introduced so far. The main idea is to give to the user the safety requirements he/she needs, and no more. We evaluated these variants on continuous control tasks such as the Linear Quadratic Gaussian control problem. Experiments showed that the algorithm effectively guarantees the imposed safety constraint and simultaneously allows for a more exploratory behaviour than na\"ive approaches. More importantly, these algorithms adapt to the environment and represent a step towards policy gradient algorithms that can be employed efficiently and safely in real applications, without requiring a strong domain knowledge nor a long period of hyper-parameter tuning. 

Future works can focus on extending the theoretical guarantees to other classes of policies (\eg softmax for discrete MDPs). This would increase the range of problems where safe reinforcement learning can be applied. Another interesting approach would be to develop new methods to automatically select other meta-parameters in SEPG, such as the batch size (as done in \cite{adaptive_batch}), or the budget-allocation strategy. The former can be easily employed with little effort. The latter requires more thought, but can yield interesting results because a different strategy can drastically change the behaviour of the algorithm. 
Moreover, a better strategy can solve the 'large budget' problem. This problem is as follows: 
when we raise the budget through our improvements in the policy performance, we are likely to spend it in exploration. However, when this is not possible, or have marginal effects, we remain with a high positive budget. While this can seem a good thing, it can cause problems, since we will make large bounded-worsening updates that destabilize the algorithm and are likely to incur in overshooting.
Two possible ways of tackling this problem are: i) discount the budget with a constant factor $\gamma$. This, however, introduces a new hyper-parameter that is domain dependent. ii) reset the budget to 0 (or a constant value) 'once in a while'. The meaning behind this approach is to make a checkpoint of the current policy performance. In fact, by resetting the budget, we are basically restarting the algorithm and setting the initial policy to be the current one. Choosing when to make a checkpoint can be a non-trivial task: too many checkpoints can drastically reduce convergence speed, while making too few can still suffer from the 'high budget' problem. 


Finally, another possible extension to this work can be to study new exploration techniques that safely explore the environment. For example, we can reduce the amount of exploration when we think we have collected enough samples from the environment, increasing the performance in an online scenario. A different approach would be to adapt exploration for each state: we can increase the exploration of states we visited less frequently, while we can exploit the states we visited more often. However, as we have seen, the exploration-exploitation dilemma is a really challenging task. There is not a general solution for this problem yet, so we need to keep pushing in this direction and devise better solutions.
