\chapter*{Estratto in lingua italiana}
%\thispagestyle{empty}
\addcontentsline{toc}{chapter}{Estratto in lingua italiana}

Ogni giorno dobbiamo compiere delle scelte: scegliamo quali vestiti indossare, se è più conveniente prendere l'autobus o la metropolitana, se impostare la sveglia per il giorno successivo e così via. Abbiamo imparato come affrontare questi problemi da quando eravamo bambini. Il processo che ci ha portato ad acquisire questa capacità è lungo e complesso, ma può essere suddiviso in pochi elementi essenziali: i) abbiamo una percezione di ciò che ci circonda, ii) possiamo interagire con l'ambiente compiendo delle azioni e iii) l'ambiente reagisce dandoci una nuova percezione. Questi tre fattori entrano sempre in gioco quando vogliamo apprendere una nuova abilità (\eg giocare a tennis). 
L'interazione con ciò che ci circonda costituisce ciò che chiamiamo esperienza. Tuttavia, l'esperienza da sola non è sufficiente: dobbiamo anche identificare un obiettivo. In termini molto semplici, possiamo dire di essere felici se l'obiettivo è stato raggiunto, altrimenti siamo tristi. La quantità di felicità (o tristezza) che otteniamo può essere vista come una ricompensa che l'ambiente ci offre. Ciò che istintivamente impariamo a fare è sviluppare delle strategie che aumentino la quantità di felicità ottenuta tramite la nostra interazione con l'ambiente. Un esempio molto calzante è dato dal processo di addestramento dei cani: il cane impara a stare seduto perché sa che poi riceverà una ricompensa.\\
Questo processo di apprendimento, quando applicato a programmi informatici, prende il nome di apprendimento per rinforzo.



L'apprendimento per rinforzo (RL) è un campo dell'intelligenza artificiale che imita il modo in cui gli esseri viventi apprendono in natura. Questi algoritmi di apprendimento si basano su un modello matematico semplificato, ma potente, che è in grado di descrivere le interazioni tra un agente e l'ambiente che lo circonda.
%
%In questo lavoro esploreremo l'approccio computazionale all'apprendimento per rinforzo, considerando un modello matematico semplificato ma potente.
Il risultato finale è una funzione che associa, ad ogni possible situazione, l'azione che l'agente deve compiere per massimizzare la ricompensa che potrà ricevere.


Queste ricompense hanno lo scopo di fornire informazioni su \textit{quale} è l'obiettivo da perseguire ma non su \textit{come} fare per raggiungerlo. All'agente non viene detto quali azioni intraprendere, piuttosto, deve scoprire da solo quali azioni sono le più redditizie. 
L'agente può imparare il valore delle proprie azioni procedendo per tentativi e accumulando esperienza.\\
Questo non è un compito banale, dal momento che le azioni possono potenzialmente avere effetti a lungo termine. Infatti, un'azione influisce sullo stato successivo, da cui un'altra azione può condurre a un altro stato ancora e così via, generando una sequenza di eventi causalmente correlati. Le ricompense a lungo termine e la ricerca per tentativi sono i due aspetti più importanti dell'apprendimento per rinforzo, che rendono questo problema difficile ma intrigante.

Tra le sfide attuali nell'ambito dell'apprendimento per rinforzo, compare il problema di come esplorare l'ambiente e di come farlo in modo sicuro. L'esplorazione è un aspetto fondamentale dell'apprendimento, poiché risolvere efficacemente un compito difficile richiede anche creatività. In questo contesto, esplorare significa come scegliere azioni che sono diverse da quelle a cui l'agente è abituato. Cambiare leggermente il modo in cui facciamo qualcosa può aiutarci ad acquisire una maggiore conoscenza dell'ambiente e, potenzialmente, aiutarci a trovare nuove soluzioni. Tuttavia, un eccesso di esplorazione non è sempre utile e può danneggiare macchine e persone, oltre ad avere un possibile impatto economico. Ad esempio, possiamo pensare a un braccio robotico che sta imparando a colpire una palla con una mazza da baseball. Può accadere che, durante la fase di apprendimento, il braccio robotico agiti la mazza con movimenti bruschi, i quali potrebbero potenzialmente portare ad una rottura. Questo problema si riflette economicamente anche nei costi di riparazione del braccio robotico.

In questa tesi vogliamo combinare questi due aspetti contrastanti di sicurezza durante l'apprendimento ed esplorazione, ed escogitare una soluzione generale che possa essere adattata alle esigenze dell'utente. Ci concentreremo su una particolare classe di algoritmi di apprendimento, chiamata 'policy gradient'~\cite{peters}. Oltre ad avere buone proprietà di convergenza, questi metodi sono particolarmente adatti per i problemi di controllo con azioni continue, in quanto sono in grado di risolvere molte delle difficoltà che si presentano in questo campo, come lo spazio di stati multidimensionale e sensori non affidabili. Inoltre, consentono di integrare facilmente delle conoscenze pregresse sul problema, al fine di progettare soluzioni più sicure ed efficaci. Il lavoro fatto recentemente nel campo dell'apprendimento per rinforzo sicuro ha portato a buoni risultati teorici. In particolare faremo riferimento al lavoro di Kakade e Langford~\cite{Kakade02approximatelyoptimal} su 'conservative policy iteration', successivamente perfezionato da Pirotta et al. \cite{safe_iteration} e adattato al metodo policy gradient in ~\cite{adaptive_step}. Tuttavia, questi risultati non considerano il fattore esplorativo, che è di grande importanza nell'apprendimento. Inoltre, fanno riferimento al solo caso, limitato, di miglioramenti monotoni, in cui l'obiettivo principale è migliorare la soluzione corrente ad ogni iterazione. Questo vincolo ha un forte impatto sulla velocità di apprendimento, dal momento che dobbiamo fare aggiornamenti molto piccoli per migliorare in modo sicuro la soluzione corrente. Per questo motivo, i metodi introdotti precedentemente sono noti per essere eccessivamente cauti, in quanto sacrificano troppo in termini di velocità di apprendimento ed esplorazione.

Partendo da questi risultati, introdurremo un nuovo framework per l'apprendimento per rinforzo sicuro. Questo framework va oltre il singolo caso di miglioramento monotono, e include diversi tipi di vincoli, ciascuno corrispondente a una diversa esigenza pratica. L'utente, quindi, potrà scegliere il vincolo di sicurezza che meglio si adatta alle sue esigenze, senza garantire più di quanto sia necessario. All'interno di questo framework, estenderemo i risultati di Pirotta et al.~\cite{adaptive_step} per occuparci anche dell'esplorazione adattiva. Il contributo finale che presenteremo in questo lavoro è un algoritmo generale denominato Safely-Exploring Policy Gradient (SEPG), che combina i nuovi risultati sviluppati in questa tesi. SEPG può essere personalizzato per soddisfare i diversi vincoli di sicurezza descritti nel nuovo framework appena introdotto.